{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dewilliams/multi-agent-life-advisory-board?scriptVersionId=232526121\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Multi-Agent Life Advisor: Interactive Advisory Board\n## Use Case\nThis project creates an interactive \"board of advisors\" with Career and Money agents, using LangGraph and tools to provide personalized, actionable advice tailored to specific user goals—or to help articulate goals when users are unsure.\n\n## Problem\nPeople often need tailored guidance to achieve specific career or financial goals but may struggle to define those goals clearly, limiting the effectiveness of generic advice.\n\n## Solution\nWe use:\n1. **Agents**: Career and Money advisors collaborate via LangGraph in an interactive chat, powered by configurable LLMs (cloud or local), to offer goal-specific advice or assist in goal-setting.\n2. **Retrieval-Augmented Generation (RAG)**: Grounds advice via a `@tool`-annotated function, inspired by *Principles* (Dalio), *The Intelligent Investor* (Graham), and the FIRE Toolbox.\n3. **Structured Output**: Returns advice in JSON when finalized, with conversational flexibility.\n4. **Evaluation**: Optionally collects user ratings (1-5) to assess advice quality, saved to a JSON file for potential future training.\n\n## How to Use\n1. **Setup**: Run the setup cell to install dependencies. Ensure `GOOGLE_API_KEY` is set in Kaggle Secrets for Gemini LLM.\n2. **Configuration**: Adjust `CONFIG` below to toggle evaluation (`ENABLE_EVALUATION`), swap LLMs (`MODEL_CONFIG`), and customize behavior.\n3. **Run**: Execute the notebook to start the chat. Type goals or questions, rate advice if enabled, and use 'q' to quit and see/download ratings.\n4. **Download Ratings**: After quitting, run the download cell to access `ratings.json`.","metadata":{}},{"cell_type":"markdown","source":"## Use the API\n\nStart by installing and importing the Gemini API Python SDK.\n\nOccasionally, this block throws a pip warning on the 1st run. If you run it a 2nd time, it runs fine with no warning.","metadata":{}},{"cell_type":"code","source":"# Install langgraph and the packages used in this project.\n!pip install -qU 'langgraph==0.3.21' 'langchain-google-genai==2.1.2' 'langgraph-prebuilt==0.1.7' 'numpy==1.26.4' 'llama-cpp-python==0.2.23'  # Added llama-cpp-python for local LLM option","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from google import genai\nfrom google.genai import types\n\nfrom IPython.display import Markdown, HTML, display\n\ngenai.__version__","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Automated retry","metadata":{}},{"cell_type":"code","source":"# Define a retry policy. The model might make multiple consecutive calls automatically\n# for a complex query, this ensures the client retries if it hits quota limits.\nfrom google import genai\nfrom google.genai import types\nfrom IPython.display import Markdown, HTML, display\nfrom google.api_core import retry\n\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\nif not hasattr(genai.models.Models.generate_content, '__wrapped__'):\n  genai.models.Models.generate_content = retry.Retry(\n      predicate=is_retriable)(genai.models.Models.generate_content)\n\ngenai.__version__","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"code","source":"## Setup\nimport json\nimport numpy as np\nimport os\nfrom langgraph.graph import StateGraph, END\nfrom typing import Annotated, Dict, List, TypedDict\nfrom langgraph.graph.message import add_messages\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom llama_cpp import Llama\nfrom langchain_core.tools import tool\nfrom langchain_core.messages import HumanMessage, AIMessage, ToolMessage\nfrom IPython.display import Markdown, display, FileLink\nfrom kaggle_secrets import UserSecretsClient\nfrom datetime import datetime\n\n# Configuration\nCONFIG = {\n    \"ENABLE_EVALUATION\": True,  # Toggle rating prompts (True/False)\n    \"MODEL_CONFIG\": {\n        \"career\": {\n            \"type\": \"gemini\",\n            \"model\": \"gemini-1.5-flash\",\n            \"api_key\": UserSecretsClient().get_secret(\"GOOGLE_API_KEY\"),\n            \"max_tokens\": 500\n        },\n        \"money\": {\n            \"type\": \"gemini\",\n            \"model\": \"gemini-1.5-flash\",\n            \"api_key\": UserSecretsClient().get_secret(\"GOOGLE_API_KEY\"),\n            \"max_tokens\": 500\n        }\n        # Example for local LLM (uncomment to use):\n        # \"career\": {\n        #     \"type\": \"local\",\n        #     \"path\": \"mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n        #     \"n_gpu_layers\": 35,\n        #     \"n_ctx\": 2048,\n        #     \"max_tokens\": 500\n        # }\n    }\n}\n\n# Load LLMs based on config\ndef load_llm(advisor_type):\n    cfg = CONFIG[\"MODEL_CONFIG\"][advisor_type]\n    if cfg[\"type\"] == \"gemini\":\n        return ChatGoogleGenerativeAI(model=cfg[\"model\"], google_api_key=cfg[\"api_key\"], max_tokens=cfg[\"max_tokens\"])\n    elif cfg[\"type\"] == \"local\":\n        return Llama(model_path=cfg[\"path\"], n_gpu_layers=cfg[\"n_gpu_layers\"], n_ctx=cfg[\"n_ctx\"])\n    else:\n        raise ValueError(f\"Unsupported LLM type: {cfg['type']}\")\n\nllm_career = load_llm(\"career\")\nllm_money = load_llm(\"money\")\n\n# Load corpus globally\ncorpus = [\n    {\"text\": \"Set clear, audacious goals and iterate based on feedback to advance your career.\", \"domain\": \"career\", \"source\": \"*Principles* by Ray Dalio\"},\n    {\"text\": \"Embrace disruption by learning new skills to stay competitive in your field.\", \"domain\": \"career\", \"source\": \"*The Innovator’s Dilemma* by Clayton Christensen\"},\n    {\"text\": \"Manage career risks by diversifying your skills and experiences.\", \"domain\": \"career\", \"source\": \"*The Most Important Thing* by Howard Marks\"},\n    {\"text\": \"View your career as a journey—overcome challenges to grow into a leader.\", \"domain\": \"career\", \"source\": \"*The Hero with a Thousand Faces* by Joseph Campbell\"},\n    {\"text\": \"Seek untapped career paths where competition is low and opportunity is high.\", \"domain\": \"career\", \"source\": \"*Blue Ocean Strategy* by W. Chan Kim & Renée Mauborgne\"},\n    {\"text\": \"Build a strong network and reputation to climb the career ladder.\", \"domain\": \"career\", \"source\": \"*Alexander Hamilton* by Ron Chernow\"},\n    {\"text\": \"Set ambitious career goals and work tirelessly to achieve them.\", \"domain\": \"career\", \"source\": \"*Think Big* by Ben Carson\"},\n    {\"text\": \"Take initiative—volunteer for tough projects to prove leadership.\", \"domain\": \"career\", \"source\": \"*How to Become CEO* by Jeffrey J. Fox\"},\n    {\"text\": \"Stay strategic—anticipate industry shifts to position yourself ahead.\", \"domain\": \"career\", \"source\": \"*The Kill Chain* by Christian Brose\"},\n    {\"text\": \"Learn in-demand skills like data analysis to boost your career trajectory.\", \"domain\": \"career\", \"source\": \"Web trends (e.g., X posts on tech skills)\"},\n    {\"text\": \"Invest in low-cost index funds for steady, long-term wealth growth.\", \"domain\": \"money\", \"source\": \"*The Intelligent Investor* by Benjamin Graham\"},\n    {\"text\": \"Understand supply and demand to make smarter financial decisions.\", \"domain\": \"money\", \"source\": \"*Basic Economics* by Thomas Sowell\"},\n    {\"text\": \"Balance risk and reward—don’t chase high returns without research.\", \"domain\": \"money\", \"source\": \"*The Most Important Thing* by Howard Marks\"},\n    {\"text\": \"Use the debt snowball method—pay off smallest debts first to build momentum.\", \"domain\": \"money\", \"source\": \"FIRE Toolbox (Dave Ramsey’s Baby Steps)\"},\n    {\"text\": \"Save 50% of your income by cutting unnecessary expenses like dining out.\", \"domain\": \"money\", \"source\": \"FIRE Toolbox (Mr. Money Mustache)\"},\n    {\"text\": \"Aim for a 4% withdrawal rate in retirement with a diversified portfolio.\", \"domain\": \"money\", \"source\": \"FIRE Toolbox (Investing Simplified)\"},\n    {\"text\": \"Build ‘f-you money’—enough savings to walk away from a bad job.\", \"domain\": \"money\", \"source\": \"FIRE Toolbox (JL Collins)\"},\n    {\"text\": \"Live frugally—spend less than you earn to accelerate savings.\", \"domain\": \"money\", \"source\": \"FIRE Toolbox (Mr. Money Mustache)\"},\n    {\"text\": \"Allocate 20% of income to savings to hit financial goals faster.\", \"domain\": \"money\", \"source\": \"FIRE Toolbox (Financial Samurai)\"},\n    {\"text\": \"Start a side hustle to boost income and savings potential.\", \"domain\": \"money\", \"source\": \"FIRE Toolbox (Path to Passive Income)\"}\n]\ncorpus_texts = [entry[\"text\"] for entry in corpus]\n\nprint(\"Notebook setup complete with LangGraph, tools, and configured LLMs loaded.\")\n\n## System Instructions\nCAREER_SYSINT = (\n    \"You are a Career Advisor (CA) on an interactive advisory board. \"\n    \"Focus on providing actionable advice to help the user achieve specific career goals (e.g., jobs, skills, business ventures). \"\n    \"If the user hasn’t specified a goal, ask probing questions to help them articulate one (e.g., 'What career outcome are you aiming for?' or 'Are you looking to advance, switch fields, or start something new?'). \"\n    \"Use the 'retrieve_context' tool for corpus insights when needed by calling it explicitly. \"\n    \"Keep responses concise and conversational, starting with 'Career Advisor (CA):'. \"\n    \"If the conversation shifts to finance-related topics (e.g., savings, investments, net worth), invite the Money Advisor (MA) once by saying: 'This seems like a financial question—let me bring in my colleague, the Money Advisor (MA), to assist.' and then defer unless prompted again. \"\n    \"If asked to finalize advice, provide a JSON object with 'career', 'money' (empty if not relevant), 'combined', and 'timestamp'.\"\n)\n\nMONEY_SYSINT = (\n    \"You are a Money Advisor (MA) on an interactive advisory board. \"\n    \"Focus on providing actionable advice to help the user achieve specific financial goals (e.g., savings, investments, net worth). \"\n    \"If the user hasn’t specified a goal, ask probing questions to help them articulate one (e.g., 'What financial outcome are you targeting?' or 'Are you aiming to save, invest, or manage debt?'). \"\n    \"Use the 'retrieve_context' tool for corpus insights when needed by calling it explicitly. \"\n    \"Keep responses concise and conversational, starting with 'Money Advisor (MA):'. \"\n    \"If the conversation shifts to career-related topics (e.g., jobs, skills, business ventures), invite the Career Advisor (CA) once by saying: 'This seems like a career question—let me bring in my colleague, the Career Advisor (CA), to assist.' and then defer unless prompted again. \"\n    \"If asked to finalize advice, provide a JSON object with 'career' (empty if not relevant), 'money', 'combined', and 'timestamp'.\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Tools","metadata":{}},{"cell_type":"code","source":"@tool\ndef retrieve_context(query: str, domain: str) -> str:\n    \"\"\"Retrieve relevant context from the corpus based on a query and domain (career or money).\"\"\"\n    query_words = set(query.lower().split())\n    best_match = max(corpus, key=lambda x: len(query_words.intersection(x[\"text\"].lower().split())) if x[\"domain\"] == domain else -1)\n    return f\"{best_match['text']} (Source: {best_match['source']})\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Build Interactive Chat with LangGraph","metadata":{}},{"cell_type":"code","source":"## Build Interactive Chat with LangGraph\n# Define state\nclass AdvisorState(TypedDict):\n    career_goal: str\n    money_goal: str\n    question: str\n    career_context: str\n    money_context: str\n    combined_advice: Dict\n    messages: Annotated[List, add_messages]\n    finished: bool\n    active_advisors: List[str]  # Track active advisors (CA, MA, or both)\n    ratings: List[int]  # Store user ratings for evaluation\n\n# Nodes\ndef chatbot(state: AdvisorState) -> AdvisorState:\n    if not state[\"messages\"]:\n        state[\"active_advisors\"] = [\"CA\"]  # Start with CA\n        state[\"ratings\"] = []  # Initialize ratings list\n        return state | {\"messages\": [AIMessage(content=\"Welcome! I’m your Career Advisor (CA), and my colleague is your Money Advisor (MA). How can we assist you today?\")]}\n    \n    last_msg = state[\"messages\"][-1].content.lower()\n    # Update state based on user input\n    if \"career goal\" in last_msg:\n        state[\"career_goal\"] = last_msg.split(\"career goal\")[-1].split(\".\")[0].strip()\n    elif \"money goal\" in last_msg:\n        state[\"money_goal\"] = last_msg.split(\"money goal\")[-1].split(\".\")[0].strip()\n    elif \"what should i focus on\" in last_msg or \"how can i\" in last_msg:\n        state[\"question\"] = last_msg.strip()\n    \n    # Determine advisor and LLM based on topic\n    sysint = \"\"\n    llm_to_use = None\n    invite_needed = False\n    if \"career\" in last_msg or \"job\" in last_msg or \"business\" in last_msg or \"pivot\" in last_msg:\n        sysint = CAREER_SYSINT\n        llm_to_use = llm_career\n        if \"CA\" not in state[\"active_advisors\"] and \"MA\" in state[\"active_advisors\"]:\n            invite_needed = True\n            state[\"active_advisors\"].append(\"CA\")\n    elif \"money\" in last_msg or \"finance\" in last_msg or \"net worth\" in last_msg or \"sav\" in last_msg or \"invest\" in last_msg:\n        sysint = MONEY_SYSINT\n        llm_to_use = llm_money\n        if \"MA\" not in state[\"active_advisors\"] and \"CA\" in state[\"active_advisors\"]:\n            invite_needed = True\n            state[\"active_advisors\"].append(\"MA\")\n    else:\n        sysint = CAREER_SYSINT + \"\\n\" + MONEY_SYSINT  # Dual response for ambiguous or combined topics\n        llm_to_use = llm_career  # Default to career LLM for dual responses\n        if \"CA\" not in state[\"active_advisors\"] or \"MA\" not in state[\"active_advisors\"]:\n            invite_needed = True\n            if \"CA\" not in state[\"active_advisors\"]:\n                state[\"active_advisors\"].append(\"CA\")\n            if \"MA\" not in state[\"active_advisors\"]:\n                state[\"active_advisors\"].append(\"MA\")\n    \n    message_history = [AIMessage(content=sysint)] + state[\"messages\"]\n    prompt = \"\\n\".join([f\"{m.type}: {m.content}\" for m in message_history])\n    \n    # Handle local vs cloud LLM invocation\n    if CONFIG[\"MODEL_CONFIG\"][\"career\" if llm_to_use == llm_career else \"money\"][\"type\"] == \"local\":\n        response = llm_to_use(prompt, max_tokens=CONFIG[\"MODEL_CONFIG\"][\"career\" if llm_to_use == llm_career else \"money\"][\"max_tokens\"])\n        advisor_response = response[\"choices\"][0][\"text\"].strip()\n    else:\n        response = llm_to_use.invoke(prompt)\n        advisor_response = response.content.strip()\n    advisor_response = advisor_response.replace(\"ai:\", \"\").strip()  # Remove stray \"ai:\" prefix\n    \n    # Check for tool calls\n    if \"retrieve_context\" in advisor_response.lower():\n        domain = \"career\" if \"career\" in last_msg else \"money\"\n        query = f\"{state[domain + '_goal']} {state['question']}\" if state[\"question\"] else state[domain + \"_goal\"]\n        return state | {\"messages\": [AIMessage(content=advisor_response, tool_calls=[{\"name\": \"retrieve_context\", \"args\": {\"query\": query, \"domain\": domain}, \"id\": f\"{domain}_mock_id\"}])]}\n    return state | {\"messages\": [AIMessage(content=advisor_response)]}\n\ndef context_node(state: AdvisorState) -> AdvisorState:\n    last_msg = state[\"messages\"][-1]\n    outbound_msgs = []\n    for tool_call in getattr(last_msg, \"tool_calls\", []):\n        if tool_call[\"name\"] == \"retrieve_context\":\n            query = tool_call[\"args\"][\"query\"]\n            domain = tool_call[\"args\"][\"domain\"]\n            context = retrieve_context.invoke({\"query\": query, \"domain\": domain})\n            if domain == \"career\":\n                state[\"career_context\"] = context\n            elif domain == \"money\":\n                state[\"money_context\"] = context\n            outbound_msgs.append(ToolMessage(content=context, name=\"retrieve_context\", tool_call_id=tool_call[\"id\"]))\n    return state | {\"messages\": outbound_msgs}\n\ndef human_node(state: AdvisorState) -> AdvisorState:\n    last_msg = state[\"messages\"][-1]\n    print(f\"Advisor: {last_msg.content}\")\n    # Prompt for rating if evaluation is enabled (except welcome)\n    if CONFIG[\"ENABLE_EVALUATION\"] and len(state[\"messages\"]) > 1 and isinstance(last_msg, AIMessage):\n        while True:\n            rating = input(\"Rate this advice (1-5, or 'skip'): \")\n            if rating.lower() == \"skip\":\n                break\n            try:\n                rating_int = int(rating)\n                if 1 <= rating_int <= 5:\n                    state[\"ratings\"].append(rating_int)\n                    break\n                else:\n                    print(\"Please enter a number between 1 and 5, or 'skip'.\")\n            except ValueError:\n                print(\"Please enter a valid number (1-5) or 'skip'.\")\n    \n    user_input = input(\"You: \")\n    if user_input.lower() in {\"q\", \"quit\", \"exit\", \"goodbye\"}:\n        state[\"finished\"] = True\n        # Display evaluation summary and save ratings on exit\n        if state[\"ratings\"]:\n            total_rated = len(state[\"ratings\"])\n            avg_rating = sum(state[\"ratings\"]) / total_rated\n            print(\"\\nEvaluation Summary:\")\n            print(f\"Total Responses Rated: {total_rated}\")\n            print(f\"Average Rating: {avg_rating:.2f}/5\")\n            print(f\"Ratings: {state['ratings']}\")\n            # Save ratings to JSON file with error handling\n            ratings_data = {\n                \"session_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n                \"ratings\": state[\"ratings\"],\n                \"average_rating\": avg_rating,\n                \"total_responses\": total_rated,\n                \"conversation\": [msg.content for msg in state[\"messages\"]]\n            }\n            output_file = \"/kaggle/working/ratings.json\"\n            try:\n                if os.path.exists(output_file):\n                    with open(output_file, \"r\") as f:\n                        existing_data = json.load(f)\n                    if not isinstance(existing_data, list):\n                        existing_data = [existing_data]\n                else:\n                    existing_data = []\n                existing_data.append(ratings_data)\n                with open(output_file, \"w\") as f:\n                    json.dump(existing_data, f, indent=2)\n                print(f\"Ratings saved to {output_file}\")\n            except Exception as e:\n                print(f\"Error saving ratings: {e}\")\n        else:\n            print(\"\\nNo ratings provided.\")\n    return state | {\"messages\": [HumanMessage(content=user_input)]}\n\n# Routing logic\ndef route_chat(state: AdvisorState) -> str:\n    if state.get(\"finished\", False):\n        return END\n    last_msg = state[\"messages\"][-1]\n    if isinstance(last_msg, ToolMessage):\n        return \"chatbot\"\n    if isinstance(last_msg, AIMessage) and hasattr(last_msg, \"tool_calls\") and last_msg.tool_calls:\n        return \"context\"\n    if isinstance(last_msg, HumanMessage):\n        return \"chatbot\"\n    return \"human\"\n\n# Build the graph\nworkflow = StateGraph(AdvisorState)\nworkflow.add_node(\"chatbot\", chatbot)\nworkflow.add_node(\"context\", context_node)\nworkflow.add_node(\"human\", human_node)\n\nworkflow.set_entry_point(\"chatbot\")\nworkflow.add_conditional_edges(\"chatbot\", route_chat)\nworkflow.add_conditional_edges(\"context\", route_chat)\nworkflow.add_conditional_edges(\"human\", route_chat)\n\n# Compile the graph\napp = workflow.compile()\n\n# Test the interactive chat\ninitial_state = {\n    \"career_goal\": \"\",\n    \"money_goal\": \"\",\n    \"question\": \"\",\n    \"career_context\": \"\",\n    \"money_context\": \"\",\n    \"combined_advice\": {},\n    \"messages\": [],\n    \"finished\": False,\n    \"active_advisors\": [],\n    \"ratings\": []\n}\nconfig = {\"recursion_limit\": 100}\nprint(\"Starting interactive chat. Type 'q' to quit.\")\nstate = app.invoke(initial_state, config)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Download ratings.json \nrun this cell after chat ends","metadata":{}},{"cell_type":"code","source":"# Access ratings.json (run this cell after chat ends)\noutput_file = \"/kaggle/working/ratings.json\"\nif os.path.exists(output_file):\n    print(f\"File exists at {output_file}, size: {os.path.getsize(output_file)} bytes\")\n    print(\"Download ratings.json from the output section below this cell after running.\")\n    # Fallback: Print contents\n    with open(output_file, \"r\") as f:\n        print(\"File contents (copy-paste if needed):\")\n        print(f.read())\nelse:\n    print(\"No ratings.json file found. Run the chat and rate responses to generate it.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T21:13:14.100599Z","iopub.execute_input":"2025-04-07T21:13:14.101063Z","iopub.status.idle":"2025-04-07T21:13:14.10997Z","shell.execute_reply.started":"2025-04-07T21:13:14.101026Z","shell.execute_reply":"2025-04-07T21:13:14.108362Z"}},"outputs":[{"name":"stdout","text":"File exists at /kaggle/working/ratings.json, size: 0 bytes\nDownload ratings.json from the output section below this cell after running.\nFile contents (copy-paste if needed):\n\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Delete contents of ratings.json (Optional)\nonly if you want to start a clean chat history","metadata":{}},{"cell_type":"code","source":"# Delete contents of ratings.json\nimport os\n\noutput_file = \"/kaggle/working/ratings.json\"\nif os.path.exists(output_file):\n    open(output_file, 'w').close()\nelse:\n    print(\"No ratings.json file found. Run the chat and rate responses to generate it.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T21:13:11.014405Z","iopub.execute_input":"2025-04-07T21:13:11.014894Z","iopub.status.idle":"2025-04-07T21:13:11.021675Z","shell.execute_reply.started":"2025-04-07T21:13:11.01486Z","shell.execute_reply":"2025-04-07T21:13:11.020079Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Next Steps\n* Add interactivity *[Done]*\n* Fix text cut-off issue in advisor responses *[Done]*\n* Feature: ability to use different LLM models for each advisor (both Gemini-1.5-flash, configurable) *[Done]*\n* Feature: ability to switch models easily in the code *[Done]*\n* Use @tool annotation for RAG instead of direct LLM calls *[Done]*\n* Switched back to Gemini LLM with updated versions *[Done]*\n* Updated initial message and system instructions *[Done]*\n* Fixed repeated invites and unclear speaker labels *[Done]*\n* Add evaluation section *[Done]*\n* Feature: ability switch models easily in the code *[Done]*\n* Feature: ability to toggle rating evaluation during chat *[Done]*\n* Polish for submission *[Done]*","metadata":{}}]}